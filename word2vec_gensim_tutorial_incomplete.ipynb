{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a2fd61-31a4-4393-b8e8-6a7a4cb75968",
   "metadata": {},
   "source": [
    "# Word2Vec before Text Clustering - Incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f652ec-6ffa-4a71-861e-15cec286dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efceb072-c442-466d-a530-e5d567ef5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab26d75-6ed6-4159-9569-5d7cc91637ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.clean.clean_data\n",
    "from src.clean.clean_data import TextCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50f3826-da45-41b7-b04d-d9021022a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat -n src/clean/clean_data.py | sed -n -e 18,90p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc01fe3-9015-4f83-b0db-fb7464bcf91e",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f67d1-9c8c-4572-92ac-d4d4d9ff2bd0",
   "metadata": {},
   "source": [
    "This is a walkthrough of using `Word2Vec` by partially following the example (of how to use `Word2Vec` on its own, without clustering) from the Gensim documentation. See the **Links** section for a link to the example from the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66edb2-07c2-4d9c-a2dd-44c0466a4987",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbca86d-019e-4709-9ff3-fc87f00712c7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], \"test\", \"test_data\")\n",
    "lee_train_file = os.path.join(test_data_dir, \"lee_background.cor\")\n",
    "lee_test_file = os.path.join(test_data_dir, \"lee.cor\")\n",
    "\n",
    "# Number of most similar training docs to show for a single inference doc\n",
    "num_most_similar_docs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668054bb-36b8-48d8-aae9-8ba1e2f26fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_words = set(stopwords.words(\"english\"))\n",
    "manual_stop_words = [\n",
    "    # HTML tags\n",
    "    \"http\",\n",
    "    \"href\",\n",
    "    \"jpg\",\n",
    "    \"imgur\",\n",
    "    \"com\",\n",
    "    \"img\",\n",
    "    \"alt\",\n",
    "    \"li\",\n",
    "    \"ul\",\n",
    "    \"ol\",\n",
    "    \"src\",\n",
    "    \"em\",\n",
    "    \"en\",\n",
    "    \"rel\",\n",
    "    \"nofollow\",\n",
    "    \"blockquote\",\n",
    "    \"www\",\n",
    "    \"png\",\n",
    "    \"aedt\",\n",
    "]\n",
    "\n",
    "# Manually add to stop words\n",
    "for manual_stop_word in manual_stop_words:\n",
    "    all_stop_words.add(manual_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0c0281-f2ed-413e-af37-fd9c5446e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_with_gensim_tags(all_tokens, tokens_only=False):\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937ccd1f-0fab-44b5-a74e-38da17ceff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_doc_tokens_word_vectors(model, doc, dim=300):\n",
    "    wvcs = []\n",
    "    for w in doc:\n",
    "        if w in model.wv.index_to_key:\n",
    "            wvcs.append(model.wv[w])\n",
    "    if not wvcs:\n",
    "        wvcs = [np.zeros(dim)]\n",
    "    return wvcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6cc583b-7e60-4768-840a-541196f50167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_doc_word(corpus, model, data, dim=300, concat_with_data=False):\n",
    "    executor = Parallel(n_jobs=cpu_count(), backend=\"multiprocessing\")\n",
    "    tasks = (\n",
    "        delayed(get_all_doc_tokens_word_vectors)(model, corpus_doc, dim)\n",
    "        for corpus_doc in corpus\n",
    "    )\n",
    "    vecs = executor(tasks)\n",
    "    corpus_word_vectors = np.array([np.mean(v, axis=0) for v in vecs])\n",
    "    df_corpus_word_vectors = pd.DataFrame(corpus_word_vectors, index=data.index)\n",
    "    assert corpus_word_vectors.shape == (data.shape[0], dim)\n",
    "    if concat_with_data:\n",
    "        data_trans = pd.concat([data, df_corpus_word_vectors], axis=1)\n",
    "        return data_trans\n",
    "    else:\n",
    "        return df_corpus_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cb6462c-914b-4647-b893-64ccd50b2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingsVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Use word embeddings to vectorize text.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        dim=300,\n",
    "        concat_with_data=False,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.concat_with_data = concat_with_data\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = gensim.models.Word2Vec(\n",
    "            sentences=X.tolist(), vector_size=self.dim, workers=1\n",
    "        )\n",
    "        self.model.build_vocab(X.tolist(), update=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # tokens_all_docs = pipe_clean.fit_transform(X).tolist()\n",
    "        # corpus = list(get_tokens_with_gensim_tags(tokens_all_docs, True))\n",
    "        X_transformed = get_avg_doc_word(\n",
    "            X.tolist(), self.model, X, self.dim, self.concat_with_data\n",
    "        )\n",
    "        return X_transformed\n",
    "        # return pd.DataFrame(np.random.rand(len(self.data), self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca802c-1f7f-4be6-9f2a-3f1e950afd80",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1996065-8a52-4c63-9377-c13a0125e0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 ms, sys: 1.74 ms, total: 2.96 ms\n",
      "Wall time: 2.39 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with smart_open.open(lee_train_file, encoding=\"iso-8859-1\") as f:\n",
    "    df_train = pd.DataFrame([line for i, line in enumerate(f)], columns=[\"text\"])\n",
    "with smart_open.open(lee_test_file, encoding=\"iso-8859-1\") as f:\n",
    "    df_test = pd.DataFrame([line for i, line in enumerate(f)], columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750aa3e-b7b4-4711-b6b0-942da88adbd8",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaaa821-f78e-43b7-b507-28a7c1a60f05",
   "metadata": {},
   "source": [
    "Define the text cleaning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd1547-a17c-42db-b86d-1d8aa83f1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_clean = Pipeline([(\"clean\", TextCleaner(\"text\", False, True, [], 2, 15))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc8d64-2c8a-44a5-8878-d74cbe5673b0",
   "metadata": {},
   "source": [
    "Clean the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03f2d9d-6f19-45a5-964e-9dfedef48168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [hundreds, of, people, have, been, forced, to,...\n",
       "1      [indian, security, forces, have, shot, dead, e...\n",
       "2      [the, national, road, toll, for, the, christma...\n",
       "3      [argentina, political, and, economic, crisis, ...\n",
       "4      [six, midwives, have, been, suspended, at, wol...\n",
       "                             ...                        \n",
       "295    [team, of, australian, and, israeli, scientist...\n",
       "296    [today, is, world, aids, day, and, the, latest...\n",
       "297    [the, federal, national, party, has, rejected,...\n",
       "298    [university, of, canberra, academic, proposal,...\n",
       "299    [australia, will, take, on, france, in, the, d...\n",
       "Name: text, Length: 300, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean = pipe_clean.fit_transform(df_train)\n",
    "train_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee01856-cfd0-40b2-b0a7-510167062961",
   "metadata": {},
   "source": [
    "Clean the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd8a75a6-4276-415a-aeb7-c9d8552a01a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [the, national, executive, of, the, strife, to...\n",
       "1    [cash, strapped, financial, services, group, a...\n",
       "2    [the, united, states, government, has, said, i...\n",
       "3    [radical, armed, islamist, group, with, ties, ...\n",
       "4    [washington, has, sharply, rebuked, russia, ov...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean = pipe_clean.fit_transform(df_test)\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07b0df-c398-45e3-a5c2-7a90e1066ebe",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4287292-31ff-479e-bc92-058afdaabf24",
   "metadata": {},
   "source": [
    "Define the text vectorization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5828354d-2184-4a46-91c5-0fefa3454163",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"vec\", WordEmbeddingsVectorizer(df_train, 300))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04073e42-dea2-4ec5-8528-e3f228d04509",
   "metadata": {},
   "source": [
    "Vectorize the cleaned training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc4966fc-b202-4871-b082-324ea3928c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 802 ms, sys: 130 ms, total: 932 ms\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015379</td>\n",
       "      <td>0.132064</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>0.249821</td>\n",
       "      <td>0.050705</td>\n",
       "      <td>-0.227408</td>\n",
       "      <td>0.094230</td>\n",
       "      <td>0.518622</td>\n",
       "      <td>0.140756</td>\n",
       "      <td>-0.073674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055642</td>\n",
       "      <td>0.410941</td>\n",
       "      <td>0.214852</td>\n",
       "      <td>-0.015370</td>\n",
       "      <td>0.435837</td>\n",
       "      <td>0.381723</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>-0.195002</td>\n",
       "      <td>0.390147</td>\n",
       "      <td>0.001138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013827</td>\n",
       "      <td>0.120411</td>\n",
       "      <td>0.120946</td>\n",
       "      <td>0.226582</td>\n",
       "      <td>0.046301</td>\n",
       "      <td>-0.207441</td>\n",
       "      <td>0.085038</td>\n",
       "      <td>0.471790</td>\n",
       "      <td>0.128211</td>\n",
       "      <td>-0.067391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050456</td>\n",
       "      <td>0.373905</td>\n",
       "      <td>0.195512</td>\n",
       "      <td>-0.014384</td>\n",
       "      <td>0.395954</td>\n",
       "      <td>0.347230</td>\n",
       "      <td>0.025046</td>\n",
       "      <td>-0.177048</td>\n",
       "      <td>0.353886</td>\n",
       "      <td>0.000755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014650</td>\n",
       "      <td>0.123485</td>\n",
       "      <td>0.125180</td>\n",
       "      <td>0.235291</td>\n",
       "      <td>0.047884</td>\n",
       "      <td>-0.212599</td>\n",
       "      <td>0.088169</td>\n",
       "      <td>0.486529</td>\n",
       "      <td>0.131155</td>\n",
       "      <td>-0.068046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052187</td>\n",
       "      <td>0.385863</td>\n",
       "      <td>0.201232</td>\n",
       "      <td>-0.014362</td>\n",
       "      <td>0.408062</td>\n",
       "      <td>0.357890</td>\n",
       "      <td>0.026131</td>\n",
       "      <td>-0.183148</td>\n",
       "      <td>0.366278</td>\n",
       "      <td>0.001784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014755</td>\n",
       "      <td>0.126605</td>\n",
       "      <td>0.125715</td>\n",
       "      <td>0.236070</td>\n",
       "      <td>0.048722</td>\n",
       "      <td>-0.215667</td>\n",
       "      <td>0.088734</td>\n",
       "      <td>0.491012</td>\n",
       "      <td>0.133693</td>\n",
       "      <td>-0.070326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052581</td>\n",
       "      <td>0.389804</td>\n",
       "      <td>0.203781</td>\n",
       "      <td>-0.014380</td>\n",
       "      <td>0.412494</td>\n",
       "      <td>0.361495</td>\n",
       "      <td>0.026240</td>\n",
       "      <td>-0.185311</td>\n",
       "      <td>0.368758</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.132882</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>0.247938</td>\n",
       "      <td>0.050812</td>\n",
       "      <td>-0.227006</td>\n",
       "      <td>0.093881</td>\n",
       "      <td>0.517039</td>\n",
       "      <td>0.140871</td>\n",
       "      <td>-0.074334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055102</td>\n",
       "      <td>0.409226</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>-0.015087</td>\n",
       "      <td>0.433899</td>\n",
       "      <td>0.380943</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>-0.194678</td>\n",
       "      <td>0.387741</td>\n",
       "      <td>0.000959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.133033</td>\n",
       "      <td>0.131424</td>\n",
       "      <td>0.246837</td>\n",
       "      <td>0.050581</td>\n",
       "      <td>-0.226320</td>\n",
       "      <td>0.093243</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.140196</td>\n",
       "      <td>-0.074045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055544</td>\n",
       "      <td>0.408626</td>\n",
       "      <td>0.213879</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>0.432522</td>\n",
       "      <td>0.379323</td>\n",
       "      <td>0.027245</td>\n",
       "      <td>-0.194437</td>\n",
       "      <td>0.386387</td>\n",
       "      <td>0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.014759</td>\n",
       "      <td>0.125671</td>\n",
       "      <td>0.126451</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.047705</td>\n",
       "      <td>-0.215873</td>\n",
       "      <td>0.089730</td>\n",
       "      <td>0.493708</td>\n",
       "      <td>0.133116</td>\n",
       "      <td>-0.070558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.390924</td>\n",
       "      <td>0.205025</td>\n",
       "      <td>-0.014976</td>\n",
       "      <td>0.414203</td>\n",
       "      <td>0.362946</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>-0.184863</td>\n",
       "      <td>0.370421</td>\n",
       "      <td>0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.016204</td>\n",
       "      <td>0.137192</td>\n",
       "      <td>0.136397</td>\n",
       "      <td>0.255157</td>\n",
       "      <td>0.052206</td>\n",
       "      <td>-0.233958</td>\n",
       "      <td>0.096799</td>\n",
       "      <td>0.532737</td>\n",
       "      <td>0.144682</td>\n",
       "      <td>-0.076510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057431</td>\n",
       "      <td>0.421997</td>\n",
       "      <td>0.220883</td>\n",
       "      <td>-0.015740</td>\n",
       "      <td>0.446918</td>\n",
       "      <td>0.391884</td>\n",
       "      <td>0.028479</td>\n",
       "      <td>-0.200539</td>\n",
       "      <td>0.399464</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.015782</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.131227</td>\n",
       "      <td>0.245762</td>\n",
       "      <td>0.049983</td>\n",
       "      <td>-0.224287</td>\n",
       "      <td>0.093026</td>\n",
       "      <td>0.511580</td>\n",
       "      <td>0.138441</td>\n",
       "      <td>-0.073207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054973</td>\n",
       "      <td>0.405275</td>\n",
       "      <td>0.212553</td>\n",
       "      <td>-0.015480</td>\n",
       "      <td>0.429807</td>\n",
       "      <td>0.376785</td>\n",
       "      <td>0.027188</td>\n",
       "      <td>-0.192400</td>\n",
       "      <td>0.384460</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.125913</td>\n",
       "      <td>0.124415</td>\n",
       "      <td>0.232635</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.213079</td>\n",
       "      <td>0.088301</td>\n",
       "      <td>0.485754</td>\n",
       "      <td>0.132251</td>\n",
       "      <td>-0.070039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052509</td>\n",
       "      <td>0.385028</td>\n",
       "      <td>0.201698</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>0.407566</td>\n",
       "      <td>0.357744</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>-0.183146</td>\n",
       "      <td>0.364412</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.015379  0.132064  0.132653  0.249821  0.050705 -0.227408  0.094230   \n",
       "1    0.013827  0.120411  0.120946  0.226582  0.046301 -0.207441  0.085038   \n",
       "2    0.014650  0.123485  0.125180  0.235291  0.047884 -0.212599  0.088169   \n",
       "3    0.014755  0.126605  0.125715  0.236070  0.048722 -0.215667  0.088734   \n",
       "4    0.015124  0.132882  0.132400  0.247938  0.050812 -0.227006  0.093881   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "295  0.015124  0.133033  0.131424  0.246837  0.050581 -0.226320  0.093243   \n",
       "296  0.014759  0.125671  0.126451  0.237189  0.047705 -0.215873  0.089730   \n",
       "297  0.016204  0.137192  0.136397  0.255157  0.052206 -0.233958  0.096799   \n",
       "298  0.015782  0.130751  0.131227  0.245762  0.049983 -0.224287  0.093026   \n",
       "299  0.014137  0.125913  0.124415  0.232635  0.047607 -0.213079  0.088301   \n",
       "\n",
       "          7         8         9    ...       290       291       292  \\\n",
       "0    0.518622  0.140756 -0.073674  ...  0.055642  0.410941  0.214852   \n",
       "1    0.471790  0.128211 -0.067391  ...  0.050456  0.373905  0.195512   \n",
       "2    0.486529  0.131155 -0.068046  ...  0.052187  0.385863  0.201232   \n",
       "3    0.491012  0.133693 -0.070326  ...  0.052581  0.389804  0.203781   \n",
       "4    0.517039  0.140871 -0.074334  ...  0.055102  0.409226  0.214100   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "295  0.515600  0.140196 -0.074045  ...  0.055544  0.408626  0.213879   \n",
       "296  0.493708  0.133116 -0.070558  ...  0.052804  0.390924  0.205025   \n",
       "297  0.532737  0.144682 -0.076510  ...  0.057431  0.421997  0.220883   \n",
       "298  0.511580  0.138441 -0.073207  ...  0.054973  0.405275  0.212553   \n",
       "299  0.485754  0.132251 -0.070039  ...  0.052509  0.385028  0.201698   \n",
       "\n",
       "          293       294       295       296       297       298       299  \n",
       "0   -0.015370  0.435837  0.381723  0.027690 -0.195002  0.390147  0.001138  \n",
       "1   -0.014384  0.395954  0.347230  0.025046 -0.177048  0.353886  0.000755  \n",
       "2   -0.014362  0.408062  0.357890  0.026131 -0.183148  0.366278  0.001784  \n",
       "3   -0.014380  0.412494  0.361495  0.026240 -0.185311  0.368758  0.000643  \n",
       "4   -0.015087  0.433899  0.380943  0.027579 -0.194678  0.387741  0.000959  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "295 -0.015003  0.432522  0.379323  0.027245 -0.194437  0.386387  0.000758  \n",
       "296 -0.014976  0.414203  0.362946  0.026128 -0.184863  0.370421  0.000487  \n",
       "297 -0.015740  0.446918  0.391884  0.028479 -0.200539  0.399464  0.000705  \n",
       "298 -0.015480  0.429807  0.376785  0.027188 -0.192400  0.384460  0.000670  \n",
       "299 -0.014044  0.407566  0.357744  0.025500 -0.183146  0.364412  0.000771  \n",
       "\n",
       "[300 rows x 300 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_wordvectors = pipe.fit_transform(train_clean)\n",
    "train_wordvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf112b-5e10-44d3-b3d8-84251988e668",
   "metadata": {},
   "source": [
    "Vectorize the cleaned testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6452eef8-ce2c-4214-8f18-a4cd8c15d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94.5 ms, sys: 61.5 ms, total: 156 ms\n",
      "Wall time: 165 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002767</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.010178</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.018911</td>\n",
       "      <td>-0.001309</td>\n",
       "      <td>-0.004781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>0.012113</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>-0.004405</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>-0.008821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003212</td>\n",
       "      <td>0.005712</td>\n",
       "      <td>-0.001433</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.010843</td>\n",
       "      <td>0.007856</td>\n",
       "      <td>0.018480</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.005060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>0.013696</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>-0.004929</td>\n",
       "      <td>0.007838</td>\n",
       "      <td>-0.008224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002954</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>-0.001137</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.009995</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>-0.000918</td>\n",
       "      <td>-0.004422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.013301</td>\n",
       "      <td>0.004430</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>-0.004263</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>-0.008277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002480</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>-0.010719</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>-0.001482</td>\n",
       "      <td>-0.004605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.013681</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.012078</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>-0.003974</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>-0.009079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003054</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>-0.000976</td>\n",
       "      <td>-0.004777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005363</td>\n",
       "      <td>0.014651</td>\n",
       "      <td>0.005819</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.012554</td>\n",
       "      <td>0.012859</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>-0.003707</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>-0.008866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.002767  0.005383 -0.001078  0.002227 -0.000012 -0.010178  0.007980   \n",
       "1 -0.003212  0.005712 -0.001433  0.001637 -0.000171 -0.010843  0.007856   \n",
       "2 -0.002954  0.005289 -0.001137  0.001585 -0.000021 -0.009995  0.008089   \n",
       "3 -0.002480  0.005009 -0.001367  0.002093  0.000117 -0.010719  0.007560   \n",
       "4 -0.003054  0.004820 -0.000597  0.001728 -0.000490 -0.011082  0.008338   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  0.018911 -0.001309 -0.004781  ...  0.005782  0.014351  0.004792  0.003489   \n",
       "1  0.018480 -0.000665 -0.005060  ...  0.005866  0.013696  0.004158  0.003206   \n",
       "2  0.017765 -0.000918 -0.004422  ...  0.005356  0.013301  0.004430  0.002472   \n",
       "3  0.018184 -0.001482 -0.004605  ...  0.005128  0.013681  0.004955  0.003049   \n",
       "4  0.019956 -0.000976 -0.004777  ...  0.005363  0.014651  0.005819  0.002919   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.012449  0.012113  0.001945 -0.004405  0.008162 -0.008821  \n",
       "1  0.011131  0.011945  0.001668 -0.004929  0.007838 -0.008224  \n",
       "2  0.011226  0.011633  0.001887 -0.004263  0.007824 -0.008277  \n",
       "3  0.012078  0.012277  0.001907 -0.003974  0.008276 -0.009079  \n",
       "4  0.012554  0.012859  0.001830 -0.003707  0.008887 -0.008866  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_wordvectors = pipe.fit_transform(test_clean)\n",
    "test_wordvectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f42045-61d2-4ca6-8b89-59c0b6ee4798",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e2002-d2b2-4bdb-afce-86e66877925a",
   "metadata": {},
   "source": [
    "1. [Gensim Docs Word2Vec tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#word2vec-demo)\n",
    "2. [Analytics Vidhya - use KMeans after Word2Vec](https://medium.com/analytics-vidhya/topic-modelling-using-word-embeddings-and-latent-dirichlet-allocation-3494778307bc)\n",
    "3. [My Space News project Topic Modeling notebook](https://github.com/edesz/miscellaneous/blob/master/links/nlp-topic-modeling/8_gensim_coherence_nlp_trials_v2.ipynb)\n",
    "   - old version with Word2Vec from SpaCy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
